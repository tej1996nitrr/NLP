{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"[('And', 'CC'),\n ('now', 'RB'),\n ('for', 'IN'),\n ('something', 'NN'),\n ('completely', 'RB'),\n ('different', 'JJ')]"},"metadata":{},"execution_count":1}],"source":["\"\"\"\n","CC-coordinating conjunction\n","RB -adverbs\n","IN-preposition\n","NN-Noun\n","JJ-adjective\n","VBP-present tense verb\n","ADJ     adjective              new, good, high, special, big, local\n","ADV     adverb              really, already, still, early, now\n","CNJ     conjunction              and, or, but, if, while, although\n","DET     determiner              the, a, some, most, every, no\n","EX     existential              there, there’s\n","FW     foreign word              dolce, ersatz, esprit, quo, maitre\n","MOD     modal verb              will, can, would, may, must, should\n","N     noun              year, home, costs, time, education\n","NP     proper noun              Alison, Africa, April, Washington\n","NUM     number              twenty-four, fourth, 1991, 14:24\n","PRO     pronoun              he, their, her, its, my, I, us\n","P     preposition              on, of, at, with, by, into, under\n","TO     the              word to to\n","UH     interjection              ah, bang, ha, whee, hmpf, oops\n","V     verb              is, has, get, do, make, see, run\n","VD     past tense              said, took, told, made, asked\n","VG     present participle              making, going, playing, working\n","VN     past participle              given, taken, begun, sung\n","WH     wh determiner              who, which, when, what, where, how\n","\n","\"\"\"\n","import nltk\n","import nltk\n","from nltk.corpus import brown\n","\n","text = nltk.word_tokenize(\"And now for something completely different\")\n","nltk.pos_tag(text)\n",""]},{"cell_type":"code","execution_count":2,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"RB: adverb\n    occasionally unabatingly maddeningly adventurously professedly\n    stirringly prominently technologically magisterially predominately\n    swiftly fiscally pitilessly ...\nJJ: adjective or numeral, ordinal\n    third ill-mannered pre-war regrettable oiled calamitous first separable\n    ectoplasmic battery-powered participatory fourth still-to-be-named\n    multilingual multi-disciplinary ...\n"}],"source":["nltk.help.upenn_tagset('RB')\n","nltk.help.upenn_tagset('JJ')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"[('They', 'PRP'),\n ('refuse', 'VBP'),\n ('to', 'TO'),\n ('permit', 'VB'),\n ('us', 'PRP'),\n ('to', 'TO'),\n ('obtain', 'VB'),\n ('the', 'DT'),\n ('refuse', 'NN'),\n ('permit', 'NN')]"},"metadata":{},"execution_count":3}],"source":["text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n","nltk.pos_tag(text)\n",""]},{"cell_type":"code","execution_count":4,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"man time day year car moment world house family child country boy\nstate job place way war girl work word\n"}],"source":["text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n","text.similar('woman')\n",""]},{"cell_type":"code","execution_count":5,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"made said done put had seen found given left heard was been brought\nset got that took in told felt\n"}],"source":["text.similar('bought')\n",""]},{"cell_type":"code","execution_count":6,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"in on to of and for with from at by that into as up out down through\nis all about\n"}],"source":["text.similar('over')\n",""]},{"cell_type":"code","execution_count":7,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"a his this their its her an that our any all one these my in your no\nsome other and\n"}],"source":["text.similar('the')\n",""]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"('fly', 'NN')"},"metadata":{},"execution_count":8}],"source":["tagged_token = nltk.tag.str2tuple('fly/NN')\n","tagged_token"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"[('The', 'AT'),\n ('grand', 'JJ'),\n ('jury', 'NN'),\n ('commented', 'VBD'),\n ('on', 'IN'),\n ('a', 'AT'),\n ('number', 'NN'),\n ('of', 'IN'),\n ('other', 'AP'),\n ('topics', 'NNS'),\n (',', ','),\n ('AMONG', 'IN'),\n ('them', 'PPO'),\n ('the', 'AT'),\n ('Atlanta', 'NP'),\n ('and', 'CC'),\n ('Fulton', 'NP-TL'),\n ('County', 'NN-TL'),\n ('purchasing', 'VBG'),\n ('departments', 'NNS'),\n ('which', 'WDT'),\n ('it', 'PPS'),\n ('said', 'VBD'),\n ('``', '``'),\n ('ARE', 'BER'),\n ('well', 'QL'),\n ('operated', 'VBN'),\n ('and', 'CC'),\n ('follow', 'VB'),\n ('generally', 'RB'),\n ('accepted', 'VBN'),\n ('practices', 'NNS'),\n ('which', 'WDT'),\n ('inure', 'VB'),\n ('to', 'IN'),\n ('the', 'AT'),\n ('best', 'JJT'),\n ('interest', 'NN'),\n ('of', 'IN'),\n ('both', 'ABX'),\n ('governments', 'NNS'),\n (\"''\", \"''\"),\n ('.', '')]"},"metadata":{},"execution_count":9}],"source":["sent = \"\"\"\n","The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN\n","other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC\n","Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS\n","said/VBD ``/`` ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB\n","accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT\n","interest/NN of/IN both/ABX governments/NNS ''/'' ./\"\"\"\n","[nltk.tag.str2tuple(t) for t in sent.split()]\n",""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"[('now', 'RB'), ('im', 'PRP'), ('left', 'VBD'), ...]"},"metadata":{},"execution_count":10}],"source":["nltk.corpus.brown.tagged_words()\n","nltk.corpus.nps_chat.tagged_words()\n",""]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"[('মহিষের', 'NN'), ('সন্তান', 'NN'), (':', 'SYM'), ...]"},"metadata":{},"execution_count":11}],"source":["nltk.corpus.indian.tagged_words()\n",""]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"['[',\n 'Alice',\n \"'\",\n 's',\n 'Adventures',\n 'in',\n 'Wonderland',\n 'by',\n 'UNK',\n 'UNK',\n 'UNK',\n 'UNK',\n 'CHAPTER',\n 'I',\n '.',\n 'Down',\n 'the',\n 'Rabbit',\n '-',\n 'UNK',\n 'Alice',\n 'was',\n 'beginning',\n 'to',\n 'get',\n 'very',\n 'tired',\n 'of',\n 'sitting',\n 'by',\n 'her',\n 'sister',\n 'on',\n 'the',\n 'bank',\n ',',\n 'and',\n 'of',\n 'having',\n 'nothing',\n 'to',\n 'do',\n ':',\n 'once',\n 'or',\n 'twice',\n 'she',\n 'had',\n 'peeped',\n 'into',\n 'the',\n 'book',\n 'her',\n 'sister',\n 'was',\n 'reading',\n ',',\n 'but',\n 'it',\n 'had',\n 'no',\n 'pictures',\n 'or',\n 'UNK',\n 'in',\n 'it',\n ',',\n \"'\",\n 'and',\n 'what',\n 'is',\n 'the',\n 'use',\n 'of',\n 'a',\n 'book',\n \",'\",\n 'thought',\n 'Alice',\n \"'\",\n 'without',\n 'pictures',\n 'or',\n 'conversation',\n \"?'\",\n 'So',\n 'she',\n 'was',\n 'considering',\n 'in',\n 'her',\n 'own',\n 'mind',\n '(',\n 'as',\n 'well',\n 'as',\n 'she',\n 'could',\n ',']"},"metadata":{},"execution_count":12}],"source":["# most common tags in the news category of the Brown Corpus\n","alice = nltk.corpus.gutenberg.words('carroll-alice.txt')\n","vocab = nltk.FreqDist(alice)\n","v1000 = list(vocab)[:1000]\n","mapping = nltk.defaultdict(lambda: 'UNK')\n","for v in v1000:\n","    mapping[v] = v\n","alice2 = [mapping[v] for v in alice]\n","alice2[:100]"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"1001"},"metadata":{},"execution_count":13}],"source":["len(set(alice2))\n",""]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["counts = nltk.defaultdict(int)\n","pos = nltk.defaultdict(lambda: nltk.defaultdict(int))\n","brown_news_tagged = brown.tagged_words(categories='news')\n","for (word, tag) in brown.tagged_words(categories='news'):\n","    counts[tag] += 1\n",""]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"6866"},"metadata":{},"execution_count":15}],"source":["counts['NP']\n",""]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"'NN'"},"metadata":{},"execution_count":16}],"source":["\n","brown_tagged_sents = brown.tagged_sents(categories='news')\n","brown_sents = brown.sents(categories='news')\n","#default tagger\n","\"\"\"Default taggers assign their tag to every single word, even words that have never been\n","encountered before. once we have processed several thousand words of\n","English text, most new words will be nouns.\n","this method performs rather poorly. On a typical corpus, it will tag\n","only about an eighth of the tokens correctly\"\"\"\n","tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n","nltk.FreqDist(tags).max()\n",""]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"[('I', 'NN'),\n ('do', 'NN'),\n ('not', 'NN'),\n ('like', 'NN'),\n ('green', 'NN'),\n ('eggs', 'NN'),\n ('and', 'NN'),\n ('ham', 'NN'),\n (',', 'NN'),\n ('I', 'NN'),\n ('do', 'NN'),\n ('not', 'NN'),\n ('like', 'NN'),\n ('them', 'NN'),\n ('Sam', 'NN'),\n ('I', 'NN'),\n ('am', 'NN'),\n ('!', 'NN')]"},"metadata":{},"execution_count":17}],"source":["raw = 'I do not like green eggs and ham, I do not like them Sam I am!'\n","tokens = nltk.word_tokenize(raw)\n","default_tagger = nltk.DefaultTagger('NN')\n","default_tagger.tag(tokens)\n",""]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"0.13089484257215028"},"metadata":{},"execution_count":18}],"source":["default_tagger.evaluate(brown_tagged_sents)\n",""]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"0.20326391789486245"},"metadata":{},"execution_count":19}],"source":["patterns = [\n"," (r'.*ing$', 'VBG'), # gerunds\n"," (r'.*ed$', 'VBD'), # simple past\n"," (r'.*es$', 'VBZ'), # 3rd singular present\n"," (r'.*ould$', 'MD'), # modals\n"," (r'.*\\'s$', 'NN$'), # possessive nouns\n"," (r'.*s$', 'NNS'), # plural nouns\n"," (r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # cardinal numbers\n"," (r'.*', 'NN') # nouns (default)\n"," ]\n","regexp_tagger = nltk.RegexpTagger(patterns)\n","regexp_tagger.tag(brown_sents[3])\n","regexp_tagger.evaluate(brown_tagged_sents)\n",""]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"0.9349006503968017"},"metadata":{},"execution_count":20}],"source":["# The Lookup Tagger\n","fd  = nltk.FreqDist(brown.words(categories='news'))\n","cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n","cfd\n","# print(cfd.conditions())\n","# print(cfd['news'])\n","most_freq_words = fd.keys()\n","likely_tags = dict((word, cfd[word].max()) for word in most_freq_words)\n","baseline_tagger = nltk.UnigramTagger(model=likely_tags)\n","baseline_tagger.evaluate(brown_tagged_sents)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"0.9349006503968017"},"metadata":{},"execution_count":21}],"source":["#N-Gram tagging\n","#training a unigram tagger, used it to tag a sentence, and then evaluate\n","brown_tagged_sents = brown.tagged_sents(categories='news')\n","brown_sents = brown.sents(categories='news')\n","unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n","unigram_tagger.tag(brown_sents[2004])\n","\n","brown_sents[0]\n","brown_tagged_sents[0]\n","unigram_tagger.evaluate(brown_tagged_sents)"]},{"cell_type":"code","execution_count":22,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"4160\n"},{"output_type":"execute_result","data":{"text/plain":"0.8121200039868434"},"metadata":{},"execution_count":22}],"source":["\"\"\"A tagger that simply memorized its\n","training data and made no attempt to construct a general model would get a perfect\n","score, but would be useless for tagging new text. Instead, we should split the data,\n","training on 90% and testing on the remaining 10%\"\"\"\n","size = int(len(brown_tagged_sents) * 0.9)\n","print(size)\n","\n","train_sents = brown_tagged_sents[:size]\n","test_sents = brown_tagged_sents[size:]\n","unigram_tagger = nltk.UnigramTagger(train_sents)\n","unigram_tagger.evaluate(test_sents)\n",""]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"[('Various', 'JJ'),\n ('of', 'IN'),\n ('the', 'AT'),\n ('apartments', 'NNS'),\n ('are', 'BER'),\n ('of', 'IN'),\n ('the', 'AT'),\n ('terrace', 'NN'),\n ('type', 'NN'),\n (',', ','),\n ('being', 'BEG'),\n ('on', 'IN'),\n ('the', 'AT'),\n ('ground', 'NN'),\n ('floor', 'NN'),\n ('so', 'CS'),\n ('that', 'CS'),\n ('entrance', 'NN'),\n ('is', 'BEZ'),\n ('direct', 'JJ'),\n ('.', '.')]"},"metadata":{},"execution_count":23}],"source":["\"\"\"An n-gram tagger is a generalization of a unigram tagger whose context is the current\n","word together with the part-of-speech tags of the n-1 preceding tokens\"\"\"\n","bigram_tagger = nltk.BigramTagger(train_sents)\n","bigram_tagger.tag(brown_sents[2007])\n",""]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"'As n gets larger, the specificity of the contexts increases, as does the chance that the\\ndata we wish to tag contains contexts that were not present in the training data. This\\nis known as the sparse data problem,'"},"metadata":{},"execution_count":24}],"source":["bigram_tagger.evaluate(test_sents) #Its overall accuracy score is very low:\n","\"\"\"As n gets larger, the specificity of the contexts increases, as does the chance that the\n","data we wish to tag contains contexts that were not present in the training data. This\n","is known as the sparse data problem,\"\"\"\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}